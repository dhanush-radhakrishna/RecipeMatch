Crawler:

The crawler's role will be to traverse the web, specifically targeting cooking blogs, Yelp pages, and cookbooks as mentioned in our proposal.
The process of "Crawling" refers to the automatic fetching of web pages, and "Controlling" involves managing the frequency and scope of the crawl to avoid overloading web servers and to ensure that only relevant pages are visited.


Page Repository / URL Database:

This is where the crawler stores the raw content fetched from the web. For RecipeMatch, this would consist of recipe details, restaurant information, and other relevant data.The URL Database keeps track of the pages that have been visited and are yet to be visited, as well as the freshness of the data, which is essential for deciding when to re-crawl pages.

The Crawling strategy:
Using Scrapy and BeautifulSoup4 to create a multithreaded scraper for each recipe website. The scraper is customized according to the website’s structure. Additionally, we use a hashmap to keep track of the websites that have been scraped to avoid scraping of duplicate pages.
HashMap structure: key: website url, value: scraped status- True/False

The data collected is cleaned to remove stop words (Eg: amounts in ingredients, etc.) and remove null values. We store the data in this model for further indexing,

Overview of the Lucene indexing strategy

Indexer:
The indexer processes the raw data stored in the Page Repository to create structured indexes. These indexes will include information such as ingredients, cuisine, cooking time, and cooking methods.
Text Structure and Utility Data Structures will be a part of this indexing process. In the context of RecipeMatch, text structure can refer to the way recipes are written, and utility structures could be used to optimize search performance or to handle synonyms (e.g., "cilantro" vs. "coriander").
Query Engine (Querying and Ranking):
Users interact with the Query Engine via the web application's querying interface. This engine processes user queries, which could include ingredient details, cuisine, or preparation time.The "Querying" function interprets user queries and fetches relevant information from the indexes.
"Ranking" involves ordering the results based on relevance. This could be determined by how well recipes match the user's ingredient list, the popularity of recipes, user ratings from Yelp, or other metrics.

Query Answer:
This is the response generated by the Query Engine, which is then presented to the user. It contains the recipes or restaurant recommendations that best match the user's search criteria.


The “create_index” function sets up a Lucene index with PyLucene from a given JSON file. It ensures the directory for the index exists, creates a Lucene store and analyzer, and configures an index writer. It defines two types of fields: one for metadata which is stored but not tokenized, and another for content that is tokenized and indexed with additional information like term frequency. The function processes each entry in the JSON file, converting lists to strings and adding them to a Lucene document with the appropriate field type. These documents are then written to the index, which is finally closed, committing the data to the index directory. This index can then be used for efficient searching of the fields like "name", "ingredients", "cuisine", or "cooking_time".


The “retrieve” function is designed to perform a search on a Lucene-indexed directory using a provided query. It utilizes a Lucene NIOFSDirectory for accessing the index, an IndexSearcher to execute the search, and a QueryParser to parse the search string, specifically targeting the 'name' field with a StandardAnalyzer. The function conducts a search for the top 10 documents matching the query, extracts each document's score and relevant fields ('name', 'cuisine', 'ingredients', 'url'), and compiles this information into a list of dictionaries, which it then returns. This is useful for applications that require quick retrieval of structured data from a large dataset based on specific search criteria.


We run the “index2.py” file by passing the sample.json as the input and indexing into the “sample_lucene_index” directory. This is done with the following command:

python3 index2.py sample.json sample_indexed_index

We run the “retrieval.py” file in order for the user to pass in the queries.Here in this above image, we can see that the user as entered “rice and dal” as the query and the code processes the query and shows all the records which contain the recipes that uses the ingredients rice and dal.

Instructions on deploying Crawler:
Run the main.py file.
You will be prompted to enter the number of pages to be crawled.
Example: python3 main.py
	Enter the number of pages to scrape:
The data will be stored in different json files (data1.json, data2.json, data3.json, data4.json, data5.json)
